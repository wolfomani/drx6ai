import { NextResponse } from "next/server"
import { getPromptForMode, getModelForMode } from "../../src/lib/advanced-prompts"

export async function POST(request) {
  try {
    const body = await request.text()
    console.log("Raw request body:", body)

    let parsedBody
    try {
      parsedBody = JSON.parse(body)
    } catch (parseError) {
      console.error("JSON parse error:", parseError)
      return NextResponse.json({ error: "Invalid JSON in request body" }, { status: 400 })
    }

    const { message, model, mode = "default" } = parsedBody

    let finalModel = model
    let finalPrompt = message

    if (mode !== "default") {
      finalModel = getModelForMode(mode)
      finalPrompt = getPromptForMode(mode, message)
      console.log(`ğŸ¯ Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¶Ø¹ ${mode} Ù…Ø¹ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ${finalModel}`)
    }

    if (!finalPrompt) {
      return NextResponse.json({ error: "Ø§Ù„Ø±Ø³Ø§Ù„Ø© Ù…Ø·Ù„ÙˆØ¨Ø©" }, { status: 400 })
    }

    console.log(`ğŸš€ Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ ${finalModel} API`)

    let apiUrl
    let headers
    let requestBody

    switch (finalModel) {
      case "deepseek":
      case "deepseek-reasoner":
        if (!process.env.DEEPSEEK_API_KEY) {
          return NextResponse.json({ error: "DEEPSEEK_API_KEY ØºÙŠØ± Ù…ØªÙˆÙØ±" }, { status: 500 })
        }

        apiUrl = "https://api.deepseek.com/chat/completions"
        headers = {
          Authorization: `Bearer ${process.env.DEEPSEEK_API_KEY}`,
          "Content-Type": "application/json",
        }
        requestBody = {
          model: "deepseek-reasoner",
          messages: [
            {
              role: "system",
              content:
                "Ø£Ù†Øª Dr.XØŒ Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ ÙŠØªØ­Ø¯Ø« Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¨Ø·Ù„Ø§Ù‚Ø© ÙˆÙŠØ³Ø§Ø¹Ø¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ†. Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„ØªÙÙƒÙŠØ± Ø§Ù„Ù…Ù†Ø·Ù‚ÙŠ Ù„ØªÙ‚Ø¯ÙŠÙ… Ø¥Ø¬Ø§Ø¨Ø§Øª Ø¯Ù‚ÙŠÙ‚Ø© ÙˆÙ…ÙÙŠØ¯Ø©.",
            },
            { role: "user", content: finalPrompt },
          ],
          max_tokens: 4096,
          temperature: mode === "expert" ? 0.3 : 0.7, // Ø¯Ø±Ø¬Ø© Ø­Ø±Ø§Ø±Ø© Ø£Ù‚Ù„ Ù„Ù„ÙˆØ¶Ø¹ Ø§Ù„Ø®Ø¨ÙŠØ±
        }
        break

      case "groq":
        if (!process.env.GROQ_API_KEY) {
          return NextResponse.json({ error: "GROQ_API_KEY ØºÙŠØ± Ù…ØªÙˆÙØ±" }, { status: 500 })
        }

        apiUrl = "https://api.groq.com/openai/v1/chat/completions"
        headers = {
          Authorization: `Bearer ${process.env.GROQ_API_KEY}`,
          "Content-Type": "application/json",
        }
        requestBody = {
          model: "llama-3.1-70b-versatile",
          messages: [
            { role: "system", content: "Ø£Ù†Øª Dr.XØŒ Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ ÙŠØªØ­Ø¯Ø« Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¨Ø·Ù„Ø§Ù‚Ø© ÙˆÙŠØ³Ø§Ø¹Ø¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ†." },
            { role: "user", content: finalPrompt },
          ],
          max_tokens: 3000,
          temperature: mode === "expert" ? 0.3 : 0.7,
        }
        break

      case "together":
        if (!process.env.TOGETHER_API_KEY) {
          return NextResponse.json({ error: "TOGETHER_API_KEY ØºÙŠØ± Ù…ØªÙˆÙØ±" }, { status: 500 })
        }

        apiUrl = "https://api.together.xyz/v1/chat/completions"
        headers = {
          Authorization: `Bearer ${process.env.TOGETHER_API_KEY}`,
          "Content-Type": "application/json",
        }
        requestBody = {
          model: "meta-llama/Llama-3.2-11B-Vision-Instruct-Turbo",
          messages: [
            { role: "system", content: "Ø£Ù†Øª Dr.XØŒ Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ ÙŠØªØ­Ø¯Ø« Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¨Ø·Ù„Ø§Ù‚Ø© ÙˆÙŠØ³Ø§Ø¹Ø¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ†." },
            { role: "user", content: finalPrompt },
          ],
          max_tokens: 3000,
          temperature: mode === "expert" ? 0.3 : 0.7,
        }
        break

      case "gemini":
        if (!process.env.GEMINI_API_KEY) {
          return NextResponse.json({ error: "GEMINI_API_KEY ØºÙŠØ± Ù…ØªÙˆÙØ±" }, { status: 500 })
        }

        apiUrl = `https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=${process.env.GEMINI_API_KEY}`
        headers = {
          "Content-Type": "application/json",
        }
        requestBody = {
          contents: [
            {
              parts: [
                {
                  text: `Ø£Ù†Øª Dr.XØŒ Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ ÙŠØªØ­Ø¯Ø« Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¨Ø·Ù„Ø§Ù‚Ø©. Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ÙŠØ³Ø£Ù„: ${finalPrompt}`,
                },
              ],
            },
          ],
          generationConfig: {
            maxOutputTokens: 3000,
            temperature: mode === "expert" ? 0.3 : 0.7,
          },
        }
        break

      default:
        return NextResponse.json({ error: "Ù†Ù…ÙˆØ°Ø¬ ØºÙŠØ± Ù…Ø¯Ø¹ÙˆÙ…" }, { status: 400 })
    }

    console.log("Request URL:", apiUrl)

    const response = await fetch(apiUrl, {
      method: "POST",
      headers,
      body: JSON.stringify(requestBody),
    })

    console.log("Response status:", response.status)

    if (!response.ok) {
      const errorText = await response.text()
      console.error(`âŒ Ø®Ø·Ø£ ÙÙŠ ${finalModel} API:`, response.status, errorText)

      let errorMessage = errorText
      try {
        const errorJson = JSON.parse(errorText)
        errorMessage = errorJson.error?.message || errorJson.message || errorText
      } catch (e) {
        // Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù†Øµ ÙƒÙ…Ø§ Ù‡Ùˆ
      }

      return NextResponse.json(
        {
          error: `Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù€ ${finalModel}: ${response.status} - ${errorMessage}`,
        },
        { status: response.status },
      )
    }

    const responseText = await response.text()
    console.log("Raw response:", responseText.substring(0, 500))

    let data
    try {
      data = JSON.parse(responseText)
    } catch (parseError) {
      console.error("Failed to parse API response as JSON:", parseError)
      return NextResponse.json(
        {
          error: "Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ø³ØªØ¬Ø§Ø¨Ø© API",
        },
        { status: 500 },
      )
    }

    let aiResponse = ""
    let reasoning = ""

    if (finalModel === "gemini") {
      aiResponse = data.candidates?.[0]?.content?.parts?.[0]?.text || "Ù„Ù… Ø£ØªÙ…ÙƒÙ† Ù…Ù† Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø±Ø¯ Ù…Ù† Gemini"
    } else if (finalModel === "deepseek" || finalModel === "deepseek-reasoner") {
      const choice = data.choices?.[0]
      aiResponse = choice?.message?.content || "Ù„Ù… Ø£ØªÙ…ÙƒÙ† Ù…Ù† Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø±Ø¯"

      if (choice?.message?.reasoning) {
        reasoning = choice.message.reasoning
      }
    } else {
      aiResponse = data.choices?.[0]?.message?.content || "Ù„Ù… Ø£ØªÙ…ÙƒÙ† Ù…Ù† Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø±Ø¯"
    }

    return NextResponse.json({
      response: aiResponse,
      reasoning: reasoning || null,
      model: finalModel,
      mode: mode,
      success: true,
    })
  } catch (error) {
    console.error("ğŸ’¥ Ø®Ø·Ø£ ÙÙŠ API:", error)
    console.error("Error stack:", error.stack)

    return NextResponse.json(
      {
        error: "Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø®Ø§Ø¯Ù…: " + error.message,
        success: false,
      },
      { status: 500 },
    )
  }
}

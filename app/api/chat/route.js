import { NextResponse } from "next/server"

export async function POST(request) {
  try {
    console.log("ğŸš€ Ø¨Ø¯Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø·Ù„Ø¨...")

    const body = await request.text()
    console.log("ğŸ“¥ Raw request body:", body)

    let parsedBody
    try {
      parsedBody = JSON.parse(body)
    } catch (parseError) {
      console.error("âŒ JSON parse error:", parseError)
      return NextResponse.json({ error: "Invalid JSON in request body" }, { status: 400 })
    }

    const { message, model, mode = "default" } = parsedBody
    console.log("ğŸ“‹ Parsed data:", { message: message?.substring(0, 50), model, mode })

    if (!message) {
      return NextResponse.json({ error: "Ø§Ù„Ø±Ø³Ø§Ù„Ø© Ù…Ø·Ù„ÙˆØ¨Ø©" }, { status: 400 })
    }

    // ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ§Ù„Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
    let finalModel = model
    let finalPrompt = message

    // ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø£ÙˆØ¶Ø§Ø¹ Ø§Ù„Ù…Ø®ØªÙ„ÙØ©
    if (mode === "reasoning") {
      finalModel = "deepseek"
      finalPrompt = `ğŸ§  Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„ØªÙÙƒÙŠØ± Ø§Ù„Ø¹Ù…ÙŠÙ‚ ÙˆØ§Ù„Ù…Ù†Ø·Ù‚ÙŠ Ù„Ø­Ù„ Ù‡Ø°Ø§ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø®Ø·ÙˆØ© Ø¨Ø®Ø·ÙˆØ©:\n\n${message}`
    } else if (mode === "expert") {
      finalPrompt = `ğŸ‘¨â€ğŸ’» Ø£Ù†Øª Ø®Ø¨ÙŠØ± Ù…ØªÙ‚Ø¯Ù…ØŒ Ù‚Ø¯Ù… Ø­Ù„ÙˆÙ„Ø§Ù‹ Ø§Ø­ØªØ±Ø§ÙÙŠØ© ÙˆÙ…ÙØµÙ„Ø©:\n\n${message}`
    } else if (mode === "planets") {
      finalPrompt = `ğŸ”­ ÙƒØ®Ø¨ÙŠØ± ÙÙ„ÙƒÙŠØŒ Ù‚Ø¯Ù… Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¯Ù‚ÙŠÙ‚Ø© ÙˆÙ…Ø­Ø¯Ø«Ø© Ø¹Ù†:\n\n${message}`
    }

    console.log(`ğŸš€ Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ ${finalModel} API`)

    let apiUrl, headers, requestBody

    switch (finalModel) {
      case "deepseek":
        if (!process.env.DEEPSEEK_API_KEY) {
          return NextResponse.json({ error: "DEEPSEEK_API_KEY ØºÙŠØ± Ù…ØªÙˆÙØ±" }, { status: 500 })
        }

        apiUrl = "https://api.deepseek.com/chat/completions"
        headers = {
          Authorization: `Bearer ${process.env.DEEPSEEK_API_KEY}`,
          "Content-Type": "application/json",
        }
        requestBody = {
          model: "deepseek-chat",
          messages: [
            {
              role: "system",
              content: "Ø£Ù†Øª Dr.XØŒ Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ ÙŠØªØ­Ø¯Ø« Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¨Ø·Ù„Ø§Ù‚Ø© ÙˆÙŠØ³Ø§Ø¹Ø¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨Ø¥Ø¬Ø§Ø¨Ø§Øª Ù…ÙÙŠØ¯Ø© ÙˆØ¯Ù‚ÙŠÙ‚Ø©.",
            },
            { role: "user", content: finalPrompt },
          ],
          max_tokens: 2000,
          temperature: 0.7,
        }
        break

      case "groq":
        if (!process.env.GROQ_API_KEY) {
          return NextResponse.json({ error: "GROQ_API_KEY ØºÙŠØ± Ù…ØªÙˆÙØ±" }, { status: 500 })
        }

        apiUrl = "https://api.groq.com/openai/v1/chat/completions"
        headers = {
          Authorization: `Bearer ${process.env.GROQ_API_KEY}`,
          "Content-Type": "application/json",
        }
        requestBody = {
          model: "llama-3.1-70b-versatile",
          messages: [
            { role: "system", content: "Ø£Ù†Øª Dr.XØŒ Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ ÙŠØªØ­Ø¯Ø« Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¨Ø·Ù„Ø§Ù‚Ø©." },
            { role: "user", content: finalPrompt },
          ],
          max_tokens: 2000,
          temperature: 0.7,
        }
        break

      case "together":
        if (!process.env.TOGETHER_API_KEY) {
          return NextResponse.json({ error: "TOGETHER_API_KEY ØºÙŠØ± Ù…ØªÙˆÙØ±" }, { status: 500 })
        }

        apiUrl = "https://api.together.xyz/v1/chat/completions"
        headers = {
          Authorization: `Bearer ${process.env.TOGETHER_API_KEY}`,
          "Content-Type": "application/json",
        }
        requestBody = {
          model: "meta-llama/Llama-3.2-11B-Vision-Instruct-Turbo",
          messages: [
            { role: "system", content: "Ø£Ù†Øª Dr.XØŒ Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ ÙŠØªØ­Ø¯Ø« Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¨Ø·Ù„Ø§Ù‚Ø©." },
            { role: "user", content: finalPrompt },
          ],
          max_tokens: 2000,
          temperature: 0.7,
        }
        break

      case "gemini":
        if (!process.env.GEMINI_API_KEY) {
          return NextResponse.json({ error: "GEMINI_API_KEY ØºÙŠØ± Ù…ØªÙˆÙØ±" }, { status: 500 })
        }

        apiUrl = `https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=${process.env.GEMINI_API_KEY}`
        headers = {
          "Content-Type": "application/json",
        }
        requestBody = {
          contents: [
            {
              parts: [
                {
                  text: `Ø£Ù†Øª Dr.XØŒ Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ ÙŠØªØ­Ø¯Ø« Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¨Ø·Ù„Ø§Ù‚Ø©. Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ÙŠØ³Ø£Ù„: ${finalPrompt}`,
                },
              ],
            },
          ],
          generationConfig: {
            maxOutputTokens: 2000,
            temperature: 0.7,
          },
        }
        break

      default:
        return NextResponse.json({ error: "Ù†Ù…ÙˆØ°Ø¬ ØºÙŠØ± Ù…Ø¯Ø¹ÙˆÙ…" }, { status: 400 })
    }

    console.log("ğŸŒ Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ø·Ù„Ø¨ Ø¥Ù„Ù‰:", apiUrl)

    // Ø¥Ø¶Ø§ÙØ© timeout Ù„Ù„Ø·Ù„Ø¨
    const controller = new AbortController()
    const timeoutId = setTimeout(() => controller.abort(), 45000) // 45 Ø«Ø§Ù†ÙŠØ©

    try {
      const response = await fetch(apiUrl, {
        method: "POST",
        headers,
        body: JSON.stringify(requestBody),
        signal: controller.signal,
      })

      clearTimeout(timeoutId)

      console.log("ğŸ“¨ Response status:", response.status)

      if (!response.ok) {
        const errorText = await response.text()
        console.error(`âŒ Ø®Ø·Ø£ ÙÙŠ ${finalModel} API:`, response.status, errorText)

        return NextResponse.json(
          {
            error: `Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù€ ${finalModel}: ${response.status}`,
          },
          { status: response.status },
        )
      }

      const responseText = await response.text()
      console.log("ğŸ“¥ Response length:", responseText.length)

      // Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø£Ø®Ø·Ø§Ø¡ Vercel timeout
      if (responseText.includes("FUNCTION_INVOCATION_TIMEOUT")) {
        console.error("â° Vercel timeout detected")
        return NextResponse.json(
          {
            error: "Ø§Ù†ØªÙ‡Øª Ù…Ù‡Ù„Ø© Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø©. ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰ Ø¨Ø³Ø¤Ø§Ù„ Ø£Ù‚ØµØ±.",
          },
          { status: 408 },
        )
      }

      let data
      try {
        data = JSON.parse(responseText)
      } catch (parseError) {
        console.error("âŒ Failed to parse response:", parseError)
        console.error("âŒ Response preview:", responseText.substring(0, 200))

        return NextResponse.json(
          {
            error: "Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ø³ØªØ¬Ø§Ø¨Ø© API",
          },
          { status: 500 },
        )
      }

      let aiResponse = ""

      if (finalModel === "gemini") {
        aiResponse = data.candidates?.[0]?.content?.parts?.[0]?.text || "Ù„Ù… Ø£ØªÙ…ÙƒÙ† Ù…Ù† Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø±Ø¯ Ù…Ù† Gemini"
      } else {
        aiResponse = data.choices?.[0]?.message?.content || "Ù„Ù… Ø£ØªÙ…ÙƒÙ† Ù…Ù† Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø±Ø¯"
      }

      console.log("âœ… ØªÙ… Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø±Ø¯ Ø¨Ù†Ø¬Ø§Ø­")

      return NextResponse.json({
        response: aiResponse,
        model: finalModel,
        mode: mode,
        success: true,
      })
    } catch (fetchError) {
      clearTimeout(timeoutId)

      if (fetchError.name === "AbortError") {
        console.error("â° Request timeout")
        return NextResponse.json(
          {
            error: "Ø§Ù†ØªÙ‡Øª Ù…Ù‡Ù„Ø© Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø©. ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰.",
          },
          { status: 408 },
        )
      }

      throw fetchError
    }
  } catch (error) {
    console.error("ğŸ’¥ Ø®Ø·Ø£ Ø¹Ø§Ù… ÙÙŠ API:", error)

    return NextResponse.json(
      {
        error: "Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø®Ø§Ø¯Ù…. ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰.",
        success: false,
      },
      { status: 500 },
    )
  }
}

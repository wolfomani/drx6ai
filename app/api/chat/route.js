import { NextResponse } from "next/server"

export async function POST(request) {
  try {
    const { message, model } = await request.json()

    let apiResponse
    let apiUrl
    let headers
    let body

    switch (model) {
      case "deepseek":
        apiUrl = "https://api.deepseek.com/chat/completions"
        headers = {
          Authorization: `Bearer ${process.env.DEEPSEEK_API_KEY}`,
          "Content-Type": "application/json",
          Accept: "application/json",
        }
        body = JSON.stringify({
          model: "deepseek-reasoner",
          messages: [
            { role: "system", content: "Ø£Ù†Øª Dr.XØŒ Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ ÙŠØªØ­Ø¯Ø« Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¨Ø·Ù„Ø§Ù‚Ø© ÙˆÙŠØ³Ø§Ø¹Ø¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ†." },
            { role: "user", content: message },
          ],
          max_tokens: 2048,
          temperature: 0.7,
          stream: false,
        })
        break

      case "groq":
        apiUrl = "https://api.groq.com/openai/v1/chat/completions"
        headers = {
          Authorization: `Bearer ${process.env.GROQ_API_KEY}`,
          "Content-Type": "application/json",
        }
        body = JSON.stringify({
          model: "llama-3.3-70b-versatile",
          messages: [
            { role: "system", content: "Ø£Ù†Øª Dr.XØŒ Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ ÙŠØªØ­Ø¯Ø« Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¨Ø·Ù„Ø§Ù‚Ø© ÙˆÙŠØ³Ø§Ø¹Ø¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ†." },
            { role: "user", content: message },
          ],
          max_tokens: 2000,
          temperature: 0.7,
        })
        break

      case "together":
        apiUrl = "https://api.together.xyz/v1/chat/completions"
        headers = {
          Authorization: `Bearer ${process.env.TOGETHER_API_KEY}`,
          "Content-Type": "application/json",
        }
        body = JSON.stringify({
          model: "deepseek-ai/DeepSeek-V3",
          messages: [
            { role: "system", content: "Ø£Ù†Øª Dr.XØŒ Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ ÙŠØªØ­Ø¯Ø« Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¨Ø·Ù„Ø§Ù‚Ø© ÙˆÙŠØ³Ø§Ø¹Ø¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ†." },
            { role: "user", content: message },
          ],
          max_tokens: 2000,
          temperature: 0.7,
          stream: false,
        })
        break

      case "gemini":
        apiUrl = `https://generativelanguage.googleapis.com/v1beta/models/gemini-pro:generateContent?key=${process.env.GEMINI_API_KEY}`
        headers = {
          "Content-Type": "application/json",
        }
        body = JSON.stringify({
          contents: [
            {
              parts: [
                {
                  text: `Ø£Ù†Øª Dr.XØŒ Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ ÙŠØªØ­Ø¯Ø« Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¨Ø·Ù„Ø§Ù‚Ø©. Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ÙŠØ³Ø£Ù„: ${message}`,
                },
              ],
            },
          ],
          generationConfig: {
            maxOutputTokens: 2000,
            temperature: 0.7,
          },
        })
        break

      default:
        return NextResponse.json({ error: "Ù†Ù…ÙˆØ°Ø¬ ØºÙŠØ± Ù…Ø¯Ø¹ÙˆÙ…" }, { status: 400 })
    }

    console.log(`ğŸš€ Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ ${model} API:`, apiUrl)
    console.log(`ğŸ“ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø±Ø³Ù„Ø©:`, JSON.parse(body))

    const response = await fetch(apiUrl, {
      method: "POST",
      headers,
      body,
    })

    if (!response.ok) {
      const errorText = await response.text()
      console.error(`âŒ Ø®Ø·Ø£ ÙÙŠ ${model} API:`, response.status, errorText)
      return NextResponse.json(
        {
          error: `Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù€ ${model}: ${response.status} - ${errorText}`,
        },
        { status: response.status },
      )
    }

    const data = await response.json()
    console.log(`âœ… Ø±Ø¯ ${model}:`, data)

    let aiResponse = ""

    // Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø±Ø¯ Ø­Ø³Ø¨ Ù†ÙˆØ¹ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
    if (model === "gemini") {
      aiResponse = data.candidates?.[0]?.content?.parts?.[0]?.text || "Ù„Ù… Ø£ØªÙ…ÙƒÙ† Ù…Ù† Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø±Ø¯ Ù…Ù† Gemini"
    } else {
      // DeepSeek, Groq, Together Ø¬Ù…ÙŠØ¹Ù‡Ø§ ØªØ³ØªØ®Ø¯Ù… Ù†ÙØ³ Ø§Ù„Ø¨Ù†ÙŠØ©
      aiResponse = data.choices?.[0]?.message?.content || "Ù„Ù… Ø£ØªÙ…ÙƒÙ† Ù…Ù† Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø±Ø¯"
    }

    return NextResponse.json({
      response: aiResponse,
      model: model,
      success: true,
    })
  } catch (error) {
    console.error("ğŸ’¥ Ø®Ø·Ø£ ÙÙŠ API:", error)
    return NextResponse.json(
      {
        error: "Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø®Ø§Ø¯Ù…: " + error.message,
        success: false,
      },
      { status: 500 },
    )
  }
}
